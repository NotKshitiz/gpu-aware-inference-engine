# ðŸš€ GPU-Aware ML Inference Engine

A scalable, hardware-aware machine learning inference engine built with **FastAPI** and **ONNX Runtime**, designed to serve transformer models with dynamic routing across **GPU**, **AMD**, or **CPU** devices. Includes intelligent request queuing, auto-retry logic, observability stack (Prometheus + Grafana + Loki), and Dockerized deployment.

---

## Features

-  Serve transformer models (e.g., GPT-2) using ONNX Runtime
-  Dynamic GPU/CPU/DirectML execution provider routing
-  Asynchronous request queue and auto-retry mechanism
-  Full observability: Prometheus for metrics, Grafana for dashboards, Loki for logs
-  Dockerized for consistent deployment across environments
-  Fault-tolerant, scalable, and ready for production use

---

##  Tech Stack

- **Languages**: Python (FastAPI), Bash
- **Inference & ML**: ONNX Runtime, Hugging Face Transformers
- **Infra**: Docker, AsyncIO, REST API
- **Monitoring**: Prometheus, Grafana, Loki

---

## ðŸ“ˆ Architecture

```plaintext
Client
  |
  â†“
FastAPI Inference API
  â”œâ”€â”€ Preprocessing / Validation
  â”œâ”€â”€ Async Queue
  â”œâ”€â”€ Retry & Timeout Logic
  â”œâ”€â”€ Execution Provider Selector (GPU / CPU / DirectML)
  â””â”€â”€ ONNX Inference Engine
       â””â”€â”€ Response
  |
  â””â”€â”€ Logs / Metrics â†’ Loki / Prometheus / Grafana

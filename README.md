#  GPU-Aware ML Inference Engine

A scalable, hardware-aware machine learning inference engine built with **FastAPI** and **ONNX Runtime**, designed to serve transformer models with dynamic routing across GPU **NVIDIA**, **AMD**, or **CPU** devices. Includes intelligent request queuing, auto-retry logic, observability stack (Prometheus + Grafana + Loki), and Dockerized deployment.

---

## Features

-  Serve transformer models (e.g., GPT-2) using ONNX Runtime
-  Dynamic GPU/CPU/ execution provider routing
-  Asynchronous request queue and auto-retry mechanism
-  Full observability: Prometheus for metrics, Grafana for dashboards, Loki for logs
-  Dockerized for consistent deployment across environments
-  Fault-tolerant, scalable, and ready for production use

---

##  Tech Stack

- **Languages**: Python (FastAPI), Bash
- **Inference & ML**: ONNX Runtime, Hugging Face Transformers
- **Infra**: Docker, AsyncIO, REST API
- **Monitoring**: Prometheus, Grafana, Loki

---

##  Architecture

```plaintext
Client
  |
  â†“
FastAPI Inference API
  â”œâ”€â”€ Preprocessing / Validation
  â”œâ”€â”€ Async Queue
  â”œâ”€â”€ Retry & Timeout Logic
  â”œâ”€â”€ Execution Provider Selector (GPU / CPU )
  â””â”€â”€ ONNX Inference Engine
       â””â”€â”€ Response
  |
  â””â”€â”€ Logs / Metrics â†’ Loki / Prometheus / Grafana
```

## Status
Project is under active development.
Initial implementation of model serving, monitoring stack, and queue-based retry system in progress.

 ðŸ§  Initial Architecture (Whiteboard Draft)

This is the first draft of the system architecture for the GPU Inference Project.

![Whiteboard Architecture](assets/architecture-whiteboard.jpg)

